{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4354b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, f1_score\n",
    "from sklearn.utils import resample\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76d85c",
   "metadata": {},
   "source": [
    "# Load Foundational Address Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e189e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"data\\20241104_layer0_sybil_features\"\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to hold individual dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through the CSV files and read each into a dataframe\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)  # Assuming the CSV files have headers\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    #df = df.drop(columns=['rank', 'num_distinct_to_addresses'])\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Data cleaning\n",
    "df = df.drop(columns=['rank'], errors='ignore')\n",
    "df['possible_triangles'] = df.in_degree * df.out_degree\n",
    "df.drop(columns=['in_degree', 'out_degree'], inplace=True, errors='ignore') # Duplicated in \"node metrics\"\n",
    "df = df[df.addr != '0x0000000000000000000000000000000000000000']\n",
    "\n",
    "df_without_addr = df.drop(columns='addr').copy()\n",
    "df_cleaned = df_without_addr.dropna(how='all').copy()\n",
    "df_cleaned['addr'] = df.loc[df_cleaned.index, 'addr'].copy()\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "df = df.drop_duplicates(subset=['addr'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43668d14",
   "metadata": {},
   "source": [
    "#  Load Labels from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7523ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"data\\20241214_labeled_addresses\"\n",
    "\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "labeled_addresses = set()\n",
    "\n",
    "# Loop through the CSV files and read each into a dataframe\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        labeled_addresses = labeled_addresses.union({line.strip() for line in f})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89dfcba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir = r\"C:\\Users\\scott\\Documents\\20240903 Octan\\20240903 Jupyter\\inputs\\20241013_hildobby_cex_evms\"\n",
    "\n",
    "\n",
    "cex_addresses = set()\n",
    "\n",
    "# Load each CSV file in the specified directory\n",
    "for filename in os.listdir(label_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(label_dir, filename)\n",
    "        # Load the CSV file\n",
    "        df_labels = pd.read_csv(file_path)\n",
    "        cex_addresses.update(df_labels['address'].unique())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540d695",
   "metadata": {},
   "source": [
    "# Load Features for Gas Provision Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7574a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory = r\"data\\20241114_gas_provision\\\\\"\n",
    "to_from_mapping = {}\n",
    "\n",
    "for csv_file in glob.glob(os.path.join(csv_directory, \"*.csv\")):\n",
    "    with open(csv_file, mode=\"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            to_node = row.get(\"activated_address\")\n",
    "            from_node = row.get(\"gas_provider\")\n",
    "            if to_node and from_node:  # Ensure neither key nor value is empty\n",
    "                to_from_mapping[to_node] = from_node\n",
    "\n",
    "df[\"gas_provider\"] = df[\"addr\"].map(to_from_mapping)\n",
    "df[\"provider_is_labeled\"] = df[\"gas_provider\"].apply(lambda gp: gp in labeled_addresses if gp else False)\n",
    "df[\"provider_is_cex\"] = df[\"gas_provider\"].apply(lambda gp: gp in cex_addresses if gp else False)\n",
    "val = set(df[\"addr\"].values)\n",
    "df[\"provider_is_interactor\"] = df[\"gas_provider\"].apply(lambda gp: gp in val if gp else False)\n",
    "df[\"provider_is_null\"] = df[\"gas_provider\"].isnull()\n",
    "df.drop(columns = ['gas_provider'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c500fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gp = None\n",
    "xs = []\n",
    "files = glob.glob(r'data\\20241117_tree_features\\*.csv')\n",
    "\n",
    "for file in files:\n",
    "    x = pd.read_csv(file)  # Assuming the CSV files have headers\n",
    "    x.columns = map(str.lower, x.columns)\n",
    "    xs.append(x)\n",
    "\n",
    "df_gp = pd.concat(xs, ignore_index=True)\n",
    "df_gp.drop(columns = 'provider_is_labeled', inplace=True) # Column is incorrect in the file, correctly computed here.\n",
    "df = df.merge(df_gp, on='addr', how='left')\n",
    "df = df.fillna(0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10a44b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = None\n",
    "xs = []\n",
    "files = glob.glob(r'data\\20250208_cex_dex_indegree\\*.csv')\n",
    "\n",
    "for file in files:\n",
    "    x = pd.read_csv(file)  # Assuming the CSV files have headers\n",
    "    x.columns = map(str.lower, x.columns)\n",
    "    xs.append(x)\n",
    "\n",
    "df_2 = pd.concat(xs, ignore_index=True)\n",
    "df = df.merge(df_2, left_on='addr', right_on='to_address')\n",
    "df.columns = map(str.lower, df.columns)\n",
    "df = df.fillna(0).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0752eb4",
   "metadata": {},
   "source": [
    "# Load Sybil Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "393c5705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gp = None\n",
    "file = r'data\\20240915_final_sybil_list\\fcfs_list.csv'\n",
    "\n",
    "address_df = pd.read_csv(file)  # Assuming the CSV files have headers\n",
    "address_df.columns = map(str.lower, address_df.columns)\n",
    "address_df['address'] = address_df['address'].str.lower()\n",
    "address_set = set(address_df['address'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a896f",
   "metadata": {},
   "source": [
    "# Add Chain Length as a Feature and Finalize Master DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29175d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNEXPECTED: provider is activated address 0x35d61a0648b972b3eb2686e36dcabaeff92b155b\n"
     ]
    }
   ],
   "source": [
    "def get_chain_length(addr, to_from_mapping, interactor_set, labeled_addresses_set):\n",
    "    \"\"\"\n",
    "    Follow the chain from `addr` upward using `to_from_mapping[child] = provider`.\n",
    "    Stop when you reach a provider that is in `labeled_addresses_set`\n",
    "    or if there's no further provider. Return the chain length if it\n",
    "    leads to a labeled address, else None.\n",
    "    \"\"\"\n",
    "    length = 0\n",
    "    interactors_in_chain = 0\n",
    "    current = addr\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        if current in interactor_set:\n",
    "            interactors_in_chain += 1\n",
    "\n",
    "        # If there's no known provider, we can't go further\n",
    "        if current not in to_from_mapping:\n",
    "            return length, interactors_in_chain\n",
    "        \n",
    "        \n",
    "        provider = to_from_mapping[current]\n",
    "        if (current == provider):\n",
    "            print (\"UNEXPECTED: provider is activated address\", current)\n",
    "            return length, interactors_in_chain\n",
    "\n",
    "        \n",
    "        length += 1\n",
    "        \n",
    "        # If the provider is labeled, we've reached the chain's start\n",
    "        if provider in labeled_addresses_set:\n",
    "            return length, interactors_in_chain\n",
    "        \n",
    "        \n",
    "        # Continue up the chain\n",
    "        current = provider\n",
    "\n",
    "labeled_addresses_set = labeled_addresses\n",
    "master_df = df.copy()\n",
    "master_df['sybil'] = master_df['addr'].isin(address_set)\n",
    "provider_addresses = set(to_from_mapping.values())\n",
    "master_df['is_provider'] = master_df['addr'].isin(provider_addresses)\n",
    "\n",
    "\n",
    "chain_lengths = []\n",
    "interactor_counts = []\n",
    "interactor_set = set(master_df.addr)\n",
    "\n",
    "for addr in master_df[\"addr\"]:\n",
    "    length, interactor_count = get_chain_length(addr, to_from_mapping, interactor_set, labeled_addresses_set)\n",
    "    chain_lengths.append(length)\n",
    "    interactor_counts.append(interactor_count)\n",
    "        \n",
    "\n",
    "master_df[\"chain_length\"] = chain_lengths\n",
    "master_df[\"interactors_in_chain\"] = interactor_counts\n",
    "\n",
    "master_df_orig = master_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b6e46",
   "metadata": {},
   "source": [
    "# Function Definitions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feeafa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_balance_data(df, label_column='sybil', test_size=0.3, val_size=0.3, random_state=42):\n",
    "    # Separate features and labels\n",
    "    X = df.drop(columns=[label_column])\n",
    "    y = df[label_column]\n",
    "    \n",
    "    # Split into initial train and test sets\n",
    "    X_train_initial, X_test, y_train_initial, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split the training set into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_initial, y_train_initial, test_size=val_size, random_state=random_state, stratify=y_train_initial\n",
    "    )\n",
    "    \n",
    "    # Combine the training features and labels for balancing\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # Separate the classes\n",
    "    majority_class = train_df[train_df[label_column] == 0]\n",
    "    minority_class = train_df[train_df[label_column] == 1]\n",
    "    \n",
    "    # Upsample the minority class\n",
    "    minority_class_upsampled = resample(\n",
    "        minority_class,\n",
    "        replace=True,\n",
    "        n_samples=len(majority_class),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Combine the balanced classes\n",
    "    balanced_train_df = pd.concat([majority_class, minority_class_upsampled])\n",
    "    \n",
    "    # Separate balanced features and labels\n",
    "    X_train_balanced = balanced_train_df.drop(columns=[label_column])\n",
    "    y_train_balanced = balanced_train_df[label_column]\n",
    "    \n",
    "    return X_train_balanced, y_train_balanced, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(X_train, y_train, X_val, y_val, selected_features, params):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train[selected_features], y_train, verbose=False)\n",
    "\n",
    "    y_probs_val = model.predict_proba(X_val[selected_features])[:, 1]\n",
    "    y_pred = (y_probs_val > 0.5).astype(int)\n",
    "\n",
    "    print(\"Confusion Matrix (Validation Set):\")\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report (Validation Set):\")\n",
    "    print(classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, y_probs_val)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    print(f\"Optimal threshold (Validation Set): {best_threshold}\")\n",
    "\n",
    "    y_pred_optimal = (y_probs_val >= best_threshold).astype(int)\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Optimal Threshold, Validation Set):\")\n",
    "    print(confusion_matrix(y_val, y_pred_optimal))\n",
    "\n",
    "    print(\"\\nClassification Report (Optimal Threshold, Validation Set):\")\n",
    "    print(classification_report(y_val, y_pred_optimal, digits=3))\n",
    "\n",
    "    print(\"Time elapsed:\", time.time() - start_time)\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_probs_val)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Precision-Recall and ROC side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axes[0].plot(recall, precision, marker='.')\n",
    "    axes[0].set_title('Precision-Recall Curve')\n",
    "    axes[0].set_xlabel('Recall')\n",
    "    axes[0].set_ylabel('Precision')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    axes[1].plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\", color='darkorange')\n",
    "    axes[1].plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Feature importances (Top 20)\n",
    "    booster = model.get_booster()\n",
    "    feature_importances = booster.get_score(importance_type='weight')\n",
    "    sorted_importances = sorted(feature_importances.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    features = [item[0] for item in sorted_importances]\n",
    "    importances = [item[1] for item in sorted_importances]\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": features,\n",
    "        \"Importance\": importances\n",
    "    })\n",
    "\n",
    "    print(\"\\nTop 20 Feature Importances:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(features, importances, color='skyblue')\n",
    "    plt.xlabel(\"Feature Importance (Weight)\", fontsize=12)\n",
    "    plt.title(\"Top 20 Feature Importances from XGBoost\", fontsize=14)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1510ea",
   "metadata": {},
   "source": [
    "# Define Model Parameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c06aaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'logloss',  \n",
    "    'n_estimators' :2000\n",
    "}\n",
    "\n",
    "selected_features = ['min_tx_value_out', \n",
    "                     'gini_coefficient', \n",
    "                     'cex_in_count', \n",
    "                     'leaf_gas_distribution_entropy', \n",
    "                     'star_like_ratio', \n",
    "                     'provider_is_star_like_attack', \n",
    "                     'leaf_gas_distribution_skewness', \n",
    "                     'interactors_in_chain', \n",
    "                     'provider_is_labeled', \n",
    "                     'provider_is_interactor', \n",
    "                     'l0_to_eth_avg_native_drop_usd', \n",
    "                     'l0_to_eth_max_native_drop_usd', \n",
    "                     'balance_factor', \n",
    "                     'l0_tx_time_span', \n",
    "                     'latest_l0_tx_time', \n",
    "                     'time_span_in', \n",
    "                     'provider_total_gas_provision_amount', \n",
    "                     'l0_avg_stargate_swap', \n",
    "                     'avg_depth', \n",
    "                     'breadth_factor', \n",
    "                     'indegree_per_block_in', \n",
    "                     'gas_distribution_skewness', \n",
    "                     'tree_size', \n",
    "                     'l0_min_stargate_swap', \n",
    "                     'provider_max_gas_provision_amount', \n",
    "                     'total_gas', \n",
    "                     'num_transactions_in', \n",
    "                     'branching_factor', \n",
    "                     'l0_to_eth_tx_time_span', \n",
    "                     'n_l0_to_eth_source_contracts', \n",
    "                     'n_l0_to_eth_projects', \n",
    "                     'provider_is_null', \n",
    "                     'max_depth', \n",
    "                     'leaf_provision_proportion', \n",
    "                     'n_l0_to_eth_project_per_source_chain', \n",
    "                     'n_l0_to_eth_txs', \n",
    "                     'earliest_l0_tx_time', \n",
    "                     'n_l0_projects', \n",
    "                     'n_l0_to_eth_dest_contracts', \n",
    "                     'provider_fan_out', \n",
    "                     'l0_to_eth_min_stargate_swap', \n",
    "                     'n_l0_source_chains', \n",
    "                     'longest_chain_ratio', \n",
    "                     'n_eth_interactions', \n",
    "                     'provider_avg_gas_provision_amount', \n",
    "                     'gas_distribution_entropy', \n",
    "                     'sparsity', \n",
    "                     'max_tx_value_out', \n",
    "                     'n_l0_source_contracts', \n",
    "                     'gas_provision_block_number', \n",
    "                     'min_tx_value_in', \n",
    "                     'tx_value_per_block_out', \n",
    "                     'chain_length', \n",
    "                     'provider_min_gas_provision_amount', \n",
    "                     'n_l0_to_eth_source_chains', \n",
    "                     'depth', \n",
    "                     'l0_to_eth_max_stargate_swap', \n",
    "                     'breadth_to_depth_ratio', \n",
    "                     'leaf_to_internal_ratio', \n",
    "                     'earliest_tx_block_in', \n",
    "                     'n_l0_project_per_source_chain', \n",
    "                     'l0_to_eth_avg_stargate_swap', \n",
    "                     'is_provider']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867aebcd",
   "metadata": {},
   "source": [
    "# Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579ed4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master_df_subset = master_df.copy()\n",
    "\n",
    "X_train_balanced, y_train_balanced, X_val, y_val, X_test, y_test = split_and_balance_data(\n",
    "    master_df_subset,\n",
    "    label_column='sybil',\n",
    "    test_size=0.3,\n",
    "    val_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "model, best_threshold = train_and_evaluate_model(\n",
    "    X_train_balanced, \n",
    "    y_train_balanced, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    selected_features, \n",
    "    params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21bb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83888350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eac07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
